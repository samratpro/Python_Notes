## Basic Request -- Requests, HTTPX, Cloudscrapper

## Request with Session

## Request with payload - Useragent, 

## Request with Authentication 

## Example SOUP

```py
from bs4 import BeautifulSoup as bs
import requests

# Basic setup: Fetching a webpage and creating a soup object
url = "https://example.com"  # Replace with your target URL
response = requests.get(url)
soup = bs(response.text, 'html.parser')

# 1. Finding a single element by tag, class, and attributes
single_element = soup.find('div', {'class': 'content', 'data-id': '123'})
if single_element:
    text = single_element.text.strip()  # Get text, remove extra whitespace
    href = single_element.get('href')  # Get attribute value (e.g., link)
    print(f"Single element text: {text}, href: {href}")

# 2. Finding all elements matching criteria
all_elements = soup.find_all('p', {'class': 'paragraph', 'lang': 'en'})
for element in all_elements:
    if element:
        text = element.text.strip()
        data_value = element.get('data-value')  # Extract custom attribute
        print(f"Element text: {text}, data-value: {data_value}")

# 3. Finding by attributes only (no tag specified)
custom_elements = soup.find_all(attrs={
    'align': 'center',
    'style': 'color: blue',
    'data-type': 'header'
})
for elem in custom_elements:
    print(f"Custom attribute element: {elem.text.strip()}")

# 4. Finding by class or ID (common for infoboxes, sidebars, etc.)
info_box = soup.find(class_='infobox vcard')  # Class-based search
info_box_id = soup.find(id='infobox')  # ID-based search
if info_box:
    print("Infobox HTML:\n", info_box.prettify())  # Pretty print HTML structure

# 5. Navigating the DOM tree
# Parent, children, and sibling navigation
first_div = soup.find('div')
if first_div:
    parent = first_div.parent  # Get parent element
    print(f"Parent tag: {parent.name}")
    
    children = first_div.find_all(recursive=False)  # Direct children only
    for child in children:
        print(f"Child tag: {child.name}, text: {child.text.strip()}")
    
    next_sibling = first_div.find_next_sibling()  # Next sibling element
    if next_sibling:
        print(f"Next sibling: {next_sibling.name}")

# 6. Using CSS selectors with select() and select_one()
# Select elements like jQuery/CSS
paragraphs = soup.select('div.content > p')  # Direct child paragraphs
for p in paragraphs:
    print(f"Selected paragraph: {p.text.strip()}")

single_h1 = soup.select_one('h1.title')  # First matching h1 with class 'title'
if single_h1:
    print(f"Single h1: {single_h1.text.strip()}")

# 7. Extracting specific attributes or nested elements
links = soup.find_all('a', href=True)  # All links with href attribute
for link in links:
    url = link.get('href')
    title = link.get('title', 'No title')  # Provide default if attribute missing
    print(f"Link: {url}, Title: {title}")

# 8. Handling nested elements (e.g., tables)
table = soup.find('table', {'class': 'data-table'})
if table:
    rows = table.find_all('tr')
    for row in rows:
        cells = row.find_all(['td', 'th'])  # Get both td and th cells
        row_data = [cell.text.strip() for cell in cells]
        print(f"Table row data: {row_data}")

# 9. Searching with regular expressions
import re
# Find elements with class containing "error" (case-insensitive)
error_elements = soup.find_all(class_=re.compile('error', re.I))
for elem in error_elements:
    print(f"Error element: {elem.text.strip()}")

# Find hrefs matching a pattern (e.g., external links)
external_links = soup.find_all('a', href=re.compile(r'^https?://'))
for link in external_links:
    print(f"External link: {link.get('href')}")

# 10. Extracting text with specific conditions
# Get text only from elements with a specific attribute value
specific_elements = soup.find_all('span', {'data-category': 'highlight'})
for elem in specific_elements:
    clean_text = ' '.join(elem.text.split())  # Normalize whitespace
    print(f"Highlighted text: {clean_text}")

# 11. Handling comments and specific tag types
from bs4 import Comment
comments = soup.find_all(string=lambda text: isinstance(text, Comment))
for comment in comments:
    print(f"HTML Comment: {comment}")

# 12. Modifying the soup (e.g., removing unwanted elements)
# Remove all script and style tags
for element in soup(['script', 'style']):
    element.decompose()  # Remove from the tree
print("Cleaned HTML (no scripts/styles):\n", soup.prettify()[:200])  # Partial output

# 13. Handling malformed HTML with different parsers
# Use 'lxml' or 'html5lib' for better handling of broken HTML
soup_lxml = bs(response.text, 'lxml')  # Faster, requires lxml package
soup_html5lib = bs(response.text, 'html5lib')  # More lenient, requires html5lib

# 14. Error handling for requests
try:
    response = requests.get(url, timeout=5)
    response.raise_for_status()  # Raise exception for bad status codes
    soup = bs(response.text, 'html.parser')
except requests.exceptions.RequestException as e:
    print(f"Error fetching URL: {e}")
```
