## Basic Request -- Requests, HTTPX, Cloudscrapper

## Request with Session

## Request with payload - Useragent, 

## Request with Authentication 

## Example SOUP
### 01. Finding a single element by tag, class, and attributes
```py
from bs4 import BeautifulSoup as bs
import requests

# Basic setup: Fetching a webpage and creating a soup object
url = "https://example.com"  # Replace with your target URL
response = requests.get(url)
soup = bs(response.text, 'html.parser')

single_element = soup.find('div', {'class': 'content', 'data-id': '123'})
if single_element:
    text = single_element.text.strip()  # Get text, remove extra whitespace
    href = single_element.get('href')  # Get attribute value (e.g., link)
    print(f"Single element text: {text}, href: {href}")

```
### 02. Finding all elements matching criteria
```py
all_elements = soup.find_all('p', {'class': 'paragraph', 'lang': 'en'})
for element in all_elements:
    if element:
        text = element.text.strip()
        data_value = element.get('data-value')  # Extract custom attribute
        print(f"Element text: {text}, data-value: {data_value}")

# 3. Finding by attributes only (no tag specified)
custom_elements = soup.find_all(attrs={
    'align': 'center',
    'style': 'color: blue',
    'data-type': 'header'
})
for elem in custom_elements:
    print(f"Custom attribute element: {elem.text.strip()}")
```
### 03. Finding by class or ID (common for infoboxes, sidebars, etc.)
```py
info_box = soup.find(class_='infobox vcard')  # Class-based search
info_box_id = soup.find(id='infobox')  # ID-based search
if info_box:
    print("Infobox HTML:\n", info_box.prettify())  # Pretty print HTML structure
```

### 04. Navigating the Tree ðŸŒ³
# Parent, children, and sibling navigation
```py
first_div = soup.find('div')
if first_div:
    parent = first_div.parent  # Get parent element
    print(f"Parent tag: {parent.name}")
    
    children = first_div.find_all(recursive=False)  # Direct children only
    for child in children:
        print(f"Child tag: {child.name}, text: {child.text.strip()}")
    
    next_sibling = first_div.find_next_sibling()  # Next sibling element
    if next_sibling:
        print(f"Next sibling: {next_sibling.name}")

element = soup.find('div', {'id': 'some_id'})
if element:
    # Find the direct parent of an element
    parent = element.find_parent('div')
    
    # Find the next element at the same level in the HTML
    next_sibling = element.find_next_sibling()
    
    # Find the previous element at the same level
    previous_sibling = element.find_previous_sibling('p') # You can specify a tag

    # Get a list of all direct children
    children = element.findChildren(recursive=False) # recursive=False gets direct children only
```
### 05. Using CSS selectors with select() and select_one() ðŸŽ¯
# Select elements like jQuery/CSS
```py
paragraphs = soup.select('div.content > p')  # Direct child paragraphs
for p in paragraphs:
    print(f"Selected paragraph: {p.text.strip()}")

single_h1 = soup.select_one('h1.title')  # First matching h1 with class 'title'
if single_h1:
    print(f"Single h1: {single_h1.text.strip()}")

# Select by tag name
paragraphs = soup.select('p')

# Select by class name
main_content = soup.select('.main-content')

# Select by ID
header = soup.select('#header')

# Select all <a> tags inside a <div>
links_in_div = soup.select('div a')

# Select by attribute value
links_to_example = soup.select('a[href="https://example.com"]')

# Get the first match (select returns a list)
first_link = soup.select_one('a') # More convenient than select('a')[0]

if first_link:
    print(first_link.get('href'))
```
### 06. Extracting specific attributes or nested elements
```py
links = soup.find_all('a', href=True)  # All links with href attribute
for link in links:
    url = link.get('href')
    title = link.get('title', 'No title')  # Provide default if attribute missing
    print(f"Link: {url}, Title: {title}")
```
### 07. Handling nested elements (e.g., tables)
```py
table = soup.find('table', {'class': 'data-table'})
if table:
    rows = table.find_all('tr')
    for row in rows:
        cells = row.find_all(['td', 'th'])  # Get both td and th cells
        row_data = [cell.text.strip() for cell in cells]
        print(f"Table row data: {row_data}")
```

### 08. Searching with regular expressions
```py
import re
# Find elements with class containing "error" (case-insensitive)
error_elements = soup.find_all(class_=re.compile('error', re.I))
for elem in error_elements:
    print(f"Error element: {elem.text.strip()}")

# Find hrefs matching a pattern (e.g., external links)
external_links = soup.find_all('a', href=re.compile(r'^https?://'))
for link in external_links:
    print(f"External link: {link.get('href')}")
```
### 09. Extracting text with specific conditions
#### Get text only from elements with a specific attribute value
```
specific_elements = soup.find_all('span', {'data-category': 'highlight'})
for elem in specific_elements:
    clean_text = ' '.join(elem.text.split())  # Normalize whitespace
    print(f"Highlighted text: {clean_text}")
```
### 10. Handling comments and specific tag types
```py
from bs4 import Comment
comments = soup.find_all(string=lambda text: isinstance(text, Comment))
for comment in comments:
    print(f"HTML Comment: {comment}")
```
### 11. Modifying the soup (e.g., removing unwanted elements)
```py
# Remove all script and style tags
for element in soup(['script', 'style']):
    element.decompose()  # Remove from the tree
print("Cleaned HTML (no scripts/styles):\n", soup.prettify()[:200])  # Partial output
```
### 12. Handling malformed HTML with different parsers
#### Use 'lxml' or 'html5lib' for better handling of broken HTML
```py
soup_lxml = bs(response.text, 'lxml')  # Faster, requires lxml package
soup_html5lib = bs(response.text, 'html5lib')  # More lenient, requires html5lib
```
#### 13. Error handling for requests
```py
try:
    response = requests.get(url, timeout=5)
    response.raise_for_status()  # Raise exception for bad status codes
    soup = bs(response.text, 'html.parser')
except requests.exceptions.RequestException as e:
    print(f"Error fetching URL: {e}")
```
