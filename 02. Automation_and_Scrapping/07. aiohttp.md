## Advance request with aiohttp
```py
import aiohttp
import asyncio
import random
import nest_asyncio
from aiohttp import CookieJar

# Apply the nest_asyncio patch
nest_asyncio.apply()

# Define a list of User-Agent strings
user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    # Add more user agents as needed
]

# Define the cookies to be used
# cookies = {
#     'cookie_name': 'cookie_value',
#     # Add more cookies as needed
# }

async def fetch(url):
    # Randomly select a User-Agent string
    headers = {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Referer': 'https://www.google.com/',
        'Connection': 'keep-alive',
        # Add more headers as needed
    }

    # Create a CookieJar with the defined cookies
    jar = CookieJar()
    # jar.update_cookies(cookies)

    async with aiohttp.ClientSession(headers=headers, cookie_jar=jar) as session:
        async with session.get(url) as response:
            # Print response data or perform other actions
            print(await response.text())

# Define the URL to visit
url = 'https://www.linkedin.com/in/samratpy/'

# Run the asynchronous fetch function using the event loop
asyncio.get_event_loop().run_until_complete(fetch(url))
```

## Multiple url
```py
import aiohttp
import asyncio
import random

# Define a list of User-Agent strings
user_agents = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    # Add more user agents as needed
]

async def fetch(session, url):
    # Randomly select a User-Agent string
    user_agent = random.choice(user_agents)
    headers = {
        'User-Agent': user_agent,
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Referer': 'https://www.google.com/',
        'Connection': 'keep-alive',
        'Accept-Encoding': 'gzip, deflate, br',
        'Cache-Control': 'no-cache',
        'Pragma': 'no-cache'
        # Add more headers as needed
    }

    # Random delay to simulate human behavior
    await asyncio.sleep(random.uniform(1, 3))

    # Make the request with custom headers
    async with session.get(url, headers=headers) as response:
        print(f"Fetching {url} with User-Agent: {user_agent}")
        return await response.text()

async def main(urls):
    # Create a session with cookie handling
    cookie_jar = aiohttp.CookieJar()
    async with aiohttp.ClientSession(cookie_jar=cookie_jar) as session:
        tasks = []
        for url in urls:
            task = asyncio.create_task(fetch(session, url))
            tasks.append(task)
        responses = await asyncio.gather(*tasks)
        for url, response in zip(urls, responses):
            print(f"Response from {url}: {response[:100]}")  # Print first 100 characters of the response

# Define a list of URLs to visit
urls = [
    'https://www.example.com/',
    'https://www.example.org/',
    'https://www.example.net/',
    # Add more URLs as needed
]

# Run the main function asynchronously
asyncio.run(main(urls))

```

## 