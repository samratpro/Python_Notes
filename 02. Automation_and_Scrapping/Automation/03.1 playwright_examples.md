## 01. Using Cookie Example Code with Chromium/Custom Chrome
```py
from time import sleep
from playwright.sync_api import sync_playwright
from playwright_stealth import stealth_sync
import os
import json


storage_state_file = os.path.join(os.getcwd(), "storage_state.json")
def is_storage_state_valid(file_path):
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
                return bool(data)  # true false depend data exist
        except json.JSONDecodeError as e:
            print(f"JSON decode error: {e}")
            return False
    else:
        open(file_path, 'w').close()  # Create an empty file
        return False

# First run: Saving session and cookies
with sync_playwright() as playwright:
    browser = playwright.chromium.launch(headless=False)
    if is_storage_state_valid(storage_state_file):
        context = browser.new_context(storage_state=storage_state_file, no_viewport=True)
    else:
        context = browser.new_context(no_viewport=True)
    page = context.new_page()
    stealth_sync(page)
    page.goto("https://www.linkedin.com/")
    input("Please log in and then press Enter to close the browser...")
    context.storage_state(path=storage_state_file)
    print("Session saved.")
    browser.close()

# Second run: Loading session and cookies
if is_storage_state_valid(storage_state_file):
    with sync_playwright() as playwright:
        browser = playwright.chromium.launch(
            headless=False,
            args=['--disable-blink-features=AutomationControlled'],
        )
        context = browser.new_context(storage_state=storage_state_file)
        page = context.new_page()
        page.goto("https://www.linkedin.com/")
        sleep(100)  # Wait to see the effect of the reused session
        browser.close()
else:
    print("Invalid or missing storage state file.")

```


## 02. Work with real browser
```py
import subprocess
import time
from typing import Optional
from playwright.sync_api import sync_playwright
chrome_process: Optional[subprocess.Popen] = None
def start_chrome():
    global chrome_process
    chrome_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
    user_data_dir = r"C:\ChromeProfile"
    chrome_process = subprocess.Popen([
        chrome_path,
        "--remote-debugging-port=9222",
        f'--user-data-dir={user_data_dir}',
        "--disable-webrtc=false",
        # "--headless=new"
        "--no-first-run", "--no-default-browser-check"  # Prevents first-run popups
    ], shell=False)
    time.sleep(1)  # add sleep is important here
    print("✅ Chrome started with remote debugging.")

# Step 2: Connect Playwright to the running Chrome and attach to the existing tab
def control_chrome():
    CDP_URL = "http://127.0.0.1:9222"

    with sync_playwright() as p:
        # Connect to the running Chrome instance
        browser = p.chromium.connect_over_cdp(CDP_URL)

        # Get the list of existing pages (tabs)
        contexts = browser.contexts
        if len(contexts) == 0:
            print("No existing tabs found. Creating a new tab.")
            page = browser.new_page()
        else:
            # Attach to the first existing tab
            page = contexts[0].pages[0]

        # Navigate to Facebook
        page.goto("https://www.facebook.com")

        # Print title to verify connection
        print("Page Title:", page.title())
        input("Press Enter to close browser")

        # Close the Playwright connection
        browser.close()

# Step 3: Close the Chrome process
def close_chrome():
    global chrome_process
    if chrome_process is not None:  # Explicit check for None
        chrome_process.terminate()  # Terminate the Chrome process
        chrome_process.wait()  # Wait for the process to terminate
        print("✅ Chrome process closed.")
    else:
        print("No Chrome process to close.")

# Run the functions
try:
    start_chrome()
    control_chrome()
finally:
    close_chrome()
```


## 03. Cookie Use with real chrome browser
```py
import subprocess
import time
import os
import json
from playwright.sync_api import sync_playwright


chrome_process = None
storage_state_file = os.path.join(os.getcwd(), "storage_state.json")
def is_storage_state_valid(file_path):
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r') as f:
                data = json.load(f)
                return bool(data)  # Return True if data exists
        except json.JSONDecodeError as e:
            print(f"JSON decode error: {e}")
            return False
    else:
        return False 

# Step 1: Start Chrome with remote debugging
def start_chrome():
    global chrome_process
    chrome_path = r"C:\Program Files\Google\Chrome\Application\chrome.exe"
    user_data_dir = r"C:\ChromeProfile"
    chrome_process = subprocess.Popen([
        chrome_path,
        "--remote-debugging-port=9222",
        f'--user-data-dir={user_data_dir}',
        "--disable-webrtc=false",
        # "--headless=new"
        "--no-first-run", "--no-default-browser-check"  # Prevents first-run popups
    ], shell=False)
    time.sleep(2)  # Add sleep to ensure Chrome starts
    print("✅ Chrome started with remote debugging.")

# Step 2: Connect Playwright to the running Chrome and attach to the existing tab
def control_chrome():
    CDP_URL = "http://127.0.0.1:9222"
    with sync_playwright() as p:
        browser = p.chromium.connect_over_cdp(CDP_URL)
        contexts = browser.contexts  # Get the list of existing pages (tabs)
        if len(contexts) == 0:
            print("No existing tabs found. Creating a new tab.")
            page = browser.new_page()
        else:
            page = contexts[0].pages[0]  # Attach to the first existing tab
        page.goto("https://www.linkedin.com/")
        input("Press Enter to save the session and close the browser...")
        storage_state = page.context.storage_state(path=storage_state_file)
        print("✅ Session saved to storage_state.json.")
        browser.close()

# Step 3: Close the Chrome process
def close_chrome():
    global chrome_process
    if chrome_process is not None:  # Explicit check for None
        chrome_process.terminate()  # Terminate the Chrome process
        chrome_process.wait()  # Wait for the process to terminate
        print("✅ Chrome process closed.")
    else:
        print("No Chrome process to close.")

# Step 4: Reuse the saved session state
def reuse_session():
    if is_storage_state_valid(storage_state_file):
        with sync_playwright() as p:
            browser = p.chromium.connect_over_cdp("http://127.0.0.1:9222")
            contexts = browser.contexts
            if len(contexts) == 0:
                print("No existing tabs found. Creating a new tab.")
                context = browser.new_context(storage_state=storage_state_file)
                page = context.new_page()
            else:
                context = contexts[0]
                page = context.pages[0]
            page.goto("https://www.linkedin.com/")
            print("✅ Navigated to LinkedIn using the saved session. now add logic here")

            # Wait to observe the effect of the reused session
            input("Press Enter to close browser")

            # Close the Playwright connection
            browser.close()
    else:
        print("Invalid or missing storage state file.")

# Run the functions
try:
    start_chrome()
    if is_storage_state_valid(storage_state_file):
        reuse_session()  # Reuse the saved session
    else:
        control_chrome()  # Save a new session
finally:
    close_chrome()
```

04. Example Amazon Product scrapping with multi tab
```py
import asyncio
from playwright.async_api import async_playwright
async def scrape_product(page, link):
    """Scrape product data from a single product page"""
    try:
        await page.goto(link)
        await page.wait_for_load_state('load')

        title = await page.locator("//span[@id='productTitle']").text_content()
        description = await page.locator("//div[@id='feature-bullets']").text_content()

        dimensions_ele = page.locator("#productDetails_techSpec_section_1")
        dimensions = await dimensions_ele.text_content() if await dimensions_ele.count() > 0 else ''

        review_ele = page.locator("(//div[@data-hook='review-collapsed'])[1]//span")
        review = await review_ele.text_content() if await review_ele.count() > 0 else ''

        return {
            "title": title.strip(),
            "description": description.strip(),
            "dimensions": dimensions.strip(),
            "reviews": review
        }
    except Exception as e:
        print(f"Error scraping {link}: {str(e)}")
        return None


async def scrape_amazon(keyword, num_tabs=5):
    """Scrape Amazon using multiple browser tabs"""
    data_list = []

    async with async_playwright() as p:
        # Launch browser
        browser = await p.chromium.launch(headless=False, args=args, executable_path="chrome/chrome.exe")
        context = await browser.new_context(storage_state='amazon_data/cookie.json', no_viewport=True)

        # Initial page for search
        search_page = await context.new_page()
        search_url = f"https://www.amazon.com/s?k={keyword.replace(' ', '+')}"
        await search_page.goto(search_url)
        await asyncio.sleep(1)

        # Get product links (first 10)
        product_links = await search_page.query_selector_all(
            "(((//div[@data-component-type='s-search-result'])//span[@data-component-type='s-product-image'])//a)"
        )
        links = []
        for e in product_links[:10]:
            try:
                href = await e.get_attribute('href')
                links.append("https://www.amazon.com" + href)
            except:
                pass

        await search_page.close()

        # Create multiple pages (tabs)
        pages = [await context.new_page() for _ in range(num_tabs)]  # Still using 3 tabs

        # Distribute scraping tasks across pages in batches of 3
        tasks = []
        for i in range(0, len(links), num_tabs):  # Step through links in batches of num_tabs (3)
            batch_links = links[i:i + num_tabs]  # Get up to 3 links at a time
            batch_tasks = []

            for j, link in enumerate(batch_links):
                page = pages[j % len(pages)]  # Use one of the 3 pages
                task = asyncio.create_task(scrape_product(page, link))
                batch_tasks.append(task)

            # Wait for this batch of 3 (or fewer) to complete
            results = await asyncio.gather(*batch_tasks)
            data_list.extend([result for result in results if result is not None])

        # Clean up
        for page in pages:
            await page.close()
        await browser.close()

    return data_list


# Run the script
keyword = "wireless mouse"
results = asyncio.run(scrape_amazon("laptop stand"))
print(results)
```
