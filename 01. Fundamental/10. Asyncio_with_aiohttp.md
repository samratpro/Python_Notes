## 01. Basic Async python
```py
import asyncio
async def task1():
    await asyncio.sleep(1)
    return "Task 1 done"

async def task2():
    await asyncio.sleep(2)
    return "Task 2 done"

async def main():
    results = await asyncio.gather(task1(), task2())
    print(results)

asyncio.run(main())  # Output: ['Task 1 done', 'Task 2 done']
```

| **Scenario**                  | **Concurrency**                     | **Control Over Tasks**               | **Performance**                         | **Use Case**                             |
|-------------------------------|-------------------------------------|--------------------------------------|------------------------------------------|------------------------------------------|
| **Without `create_task()`**    | All tasks run concurrently          | No control over individual tasks     | All tasks run as soon as `gather` is called | Good for simple concurrent I/O tasks     |
| **With `create_task()`**       | All tasks run concurrently          | Explicit task creation and control   | Similar performance, more control over tasks | Better control for complex task management |
| **With `create_task()` + `Semaphore`** | Limited number of concurrent tasks  | Control over both tasks and concurrency | Can avoid system overload and manage rate limits | Best for managing limited resources or rate-limited APIs |

## 02. async without Create_Task
```py
import asyncio
import aiohttp  # Async HTTP requests

async def fetch_url(session, url):
    async with session.get(url) as response:
        content = await response.text()
        print(f"Fetched {len(content)} characters from {url}")

async def main(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_url(session, url) for url in urls]
        await asyncio.gather(*tasks)

urls = ["https://example.com", "https://example.org", "https://example.net"]
asyncio.run(main(urls))
```
## 03. async with Create_Task
```py
import asyncio
import aiohttp

# Function to fetch a single URL
async def fetch_url(session, url):
    try:
        async with session.get(url) as response:
            content = await response.text()
            print(f"Fetched {len(content)} characters from {url}")
    except Exception as e:
        print(f"Error fetching {url}: {e}")

# Main function to fetch multiple URLs concurrently with limited concurrency
async def main(urls):

    async with aiohttp.ClientSession() as session:
        tasks = []

        # Create tasks for each URL
        for url in urls:
            task = asyncio.create_task(fetch_url(session, url))  # Schedule the task with a semaphore
            tasks.append(task)

        # Wait for all tasks to complete
        await asyncio.gather(*tasks)

# List of URLs to fetch
urls = [
    "https://example.com",
    "https://example.org",
    "https://example.net",
    "https://httpbin.org/get",
    "https://jsonplaceholder.typicode.com/posts"
]

# Run the event loop
asyncio.run(main(urls))
```
## 04. async with Create_Task with Thread Limit
```py
import asyncio
import aiohttp

TASK_THREAD_LIMIT = 3
# Function to fetch a single URL
async def fetch_url(session, url, semaphore):
    async with semaphore:
        try:
            async with session.get(url) as response:
                content = await response.text()
                print(f"Fetched {len(content)} characters from {url}")
        except Exception as e:
            print(f"Error fetching {url}: {e}")

# Main function to fetch multiple URLs concurrently with limited concurrency
async def main(urls):
    semaphore = asyncio.Semaphore(TASK_THREAD_LIMIT)  # Limit to 5 concurrent tasks

    async with aiohttp.ClientSession() as session:
        tasks = []

        # Create tasks for each URL
        for url in urls:
            task = asyncio.create_task(fetch_url(session, url, semaphore))  # Schedule the task with a semaphore
            tasks.append(task)

        # Wait for all tasks to complete
        await asyncio.gather(*tasks)

# List of URLs to fetch
urls = [
    "https://example.com",
    "https://example.org",
    "https://example.net",
    "https://httpbin.org/get",
    "https://jsonplaceholder.typicode.com/posts"
]

# Run the event loop
asyncio.run(main(urls))

```
